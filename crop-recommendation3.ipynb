{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f812497-6f84-4d33-9563-9682be2c93b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings \n",
    "  \n",
    "print('Hello') \n",
    "  \n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7da157-d884-43c8-943e-7bbc43d57f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Crop_recommendation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ad0a4-1133-4e4e-8502-51723ca712c3",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4eba85a-5d8d-4901-bc90-02a7266e79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2a1f27-5a4a-423b-a4c9-83f503185ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns='label'),\n",
    "    df['label'],\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3bb2fa-f4f8-4a66-9996-fa7266804062",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test.npy', x_test)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478eff3-2367-4d96-af27-a167b7970c4f",
   "metadata": {},
   "source": [
    "# column Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d4d5fe5-a171-4508-af9a-79fdc98c3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764d6d07-9ac3-4865-969e-c95e1f08b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf1 = ColumnTransformer(transformers=[\n",
    "    ('standarisation', StandardScaler(with_mean=False), [0,6]),\n",
    "],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0165ce-9f89-4d5a-ba32-37a03501b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf2 = ColumnTransformer(transformers=[\n",
    "    ('ohe_label', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [-1])\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83824dc4-590a-41d1-b8e6-7bcc82f2b3c7",
   "metadata": {},
   "source": [
    "# importing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5c5050-d436-4140-aa0b-b575ebe4f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  # k-Nearest Neighbors (KNN)\n",
    "from sklearn.svm import SVC  # Support Vector Machine (SVM)\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree Classifier\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
    "from sklearn.naive_bayes import GaussianNB  # Naive Bayes\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting Machine (GBM)\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3dc07a-34d8-4d6e-ac42-96a1d86bce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'RandomForestClassifier': RandomForestClassifier()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e21902d-87ee-40b1-ae44-834b501a9e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['KNeighborsClassifier', 'SVC', 'DecisionTreeClassifier', 'LogisticRegression', 'GaussianNB', 'GradientBoostingClassifier', 'RandomForestClassifier'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c1216f6-58b4-4709-bb9c-8132f6a55f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier pipeline: Pipeline(steps=[('scaler',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('standarisation',\n",
      "                                                  StandardScaler(with_mean=False),\n",
      "                                                  [0, 6])])),\n",
      "                ('model', KNeighborsClassifier())])\n",
      "SVC pipeline: Pipeline(steps=[('scaler',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('standarisation',\n",
      "                                                  StandardScaler(with_mean=False),\n",
      "                                                  [0, 6])])),\n",
      "                ('model', SVC())])\n",
      "DecisionTreeClassifier pipeline: Pipeline(steps=[('encode',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('ohe_label',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False),\n",
      "                                                  [-1])])),\n",
      "                ('model', DecisionTreeClassifier())])\n",
      "LogisticRegression pipeline: Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('encode',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('ohe_label',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False),\n",
      "                                                  [-1])])),\n",
      "                ('model', LogisticRegression())])\n",
      "GaussianNB pipeline: Pipeline(steps=[('encode',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('ohe_label',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False),\n",
      "                                                  [-1])])),\n",
      "                ('model', GaussianNB())])\n",
      "GradientBoostingClassifier pipeline: Pipeline(steps=[('encode',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('ohe_label',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False),\n",
      "                                                  [-1])])),\n",
      "                ('model', GradientBoostingClassifier())])\n",
      "RandomForestClassifier pipeline: Pipeline(steps=[('encode',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('ohe_label',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore',\n",
      "                                                                sparse_output=False),\n",
      "                                                  [-1])])),\n",
      "                ('model', RandomForestClassifier())])\n"
     ]
    }
   ],
   "source": [
    "pipelines = {\n",
    "    'KNeighborsClassifier': Pipeline([\n",
    "        ('scaler', trf1) , # StandardScaler as a preprocessing step\n",
    "        ('model', KNeighborsClassifier())\n",
    "    ]),\n",
    "    'SVC': Pipeline([\n",
    "        ('scaler', trf1),\n",
    "        ('model', SVC())\n",
    "    ]),\n",
    "    'DecisionTreeClassifier': Pipeline([\n",
    "        # No need to scale for Decision Tree, so skipping the scaler\n",
    "        ('encode', trf2),\n",
    "        ('model', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('encode', trf2),\n",
    "        ('model', LogisticRegression())\n",
    "    ]),\n",
    "    'GaussianNB': Pipeline([\n",
    "        # Naive Bayes does not require scaling, skipping the scaler\n",
    "        ('encode', trf2),\n",
    "        ('model', GaussianNB())\n",
    "    ]),\n",
    "    'GradientBoostingClassifier': Pipeline([\n",
    "        # Skipping scaler for GBM as it doesn't need feature scaling\n",
    "        ('encode', trf2),\n",
    "        ('model', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'RandomForestClassifier': Pipeline([\n",
    "        # Skipping scaler for Random Forest as it's not necessary\n",
    "        ('encode', trf2),\n",
    "        ('model', RandomForestClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Optionally, print the pipelines to check\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"{model_name} pipeline:\", pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68a1d0af-f41b-47b2-b235-e7f3a1946ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier mode with accuracy: 0.925\n",
      "SVC mode with accuracy: 0.8159090909090909\n",
      "DecisionTreeClassifier mode with accuracy: 0.9613636363636363\n",
      "LogisticRegression mode with accuracy: 0.9090909090909091\n",
      "GaussianNB mode with accuracy: 0.5704545454545454\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m pipelines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 2\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(x_train\u001b[38;5;241m.\u001b[39mvalues, y_train)\n\u001b[0;32m      3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m      4\u001b[0m     score \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:525\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    526\u001b[0m     X,\n\u001b[0;32m    527\u001b[0m     y,\n\u001b[0;32m    528\u001b[0m     raw_predictions,\n\u001b[0;32m    529\u001b[0m     sample_weight,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    531\u001b[0m     X_val,\n\u001b[0;32m    532\u001b[0m     y_val,\n\u001b[0;32m    533\u001b[0m     sample_weight_val,\n\u001b[0;32m    534\u001b[0m     begin_at_stage,\n\u001b[0;32m    535\u001b[0m     monitor,\n\u001b[0;32m    536\u001b[0m )\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:603\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    596\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    597\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    598\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    599\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    600\u001b[0m         )\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    604\u001b[0m     i,\n\u001b[0;32m    605\u001b[0m     X,\n\u001b[0;32m    606\u001b[0m     y,\n\u001b[0;32m    607\u001b[0m     raw_predictions,\n\u001b[0;32m    608\u001b[0m     sample_weight,\n\u001b[0;32m    609\u001b[0m     sample_mask,\n\u001b[0;32m    610\u001b[0m     random_state,\n\u001b[0;32m    611\u001b[0m     X_csc,\n\u001b[0;32m    612\u001b[0m     X_csr,\n\u001b[0;32m    613\u001b[0m )\n\u001b[0;32m    615\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, model in pipelines.items():\n",
    "    model.fit(x_train.values, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} mode with accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4535bda-5d52-427f-a120-7779784e5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(\"Hello\")\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f'\"{model_name}\" : \"models/{model_name}.pkl\",')\n",
    "    file = pickle.dump(pipeline, open(f'models/{model_name}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba937a-2d36-44f3-a610-372ad5118ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
